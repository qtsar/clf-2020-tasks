{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов.\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_PATH = '../data/news/'\n",
    "TRAIN_PATH = NEWS_PATH + 'news_train.txt'\n",
    "TEST_PATH = NEWS_PATH + 'news_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = list(open(TRAIN_PATH, 'r', encoding='utf-8'))\n",
    "test_lines = list(open(TEST_PATH, 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,\n",
       " ['science',\n",
       "  'Boeing займется техобслуживанием тайваньских \"Апачей\"',\n",
       "  'Американский авиастроительный концерн Boeing получил контракт на техническое обслуживание ударных вертолетов AH-64D Apache, которые в перспективе должны получить сухопутные войска Тайваня. Как сообщает Defense Aerospace, сумма сделки составила 141,3 миллиона долларов. Речь о идет о техническом обслуживании 30 машин. Подробности соглашения не раскрываются. Как ожидается, работы по контракту завершатся в декабре 2017 года.Тайвань заказал у США 30 вертолетов Apache в 2008 году. Помимо этих машин в заказ также вошла поставка многоцелевых вертолетов Black Hawk, зенитных ракетных комплексов Patriot и истребителей F-16. В начале 2010 года стало известно, что США частично одобрили продажу военной техники Тайваню. В частности, было решено поставить стране комплексы Patriot, вертолеты Black Hawk и оборудование для связи.Считалось, что поставка AH-64D Тайваню была заблокирована, однако в 2010 году стало известно, что США все же решили продать Тайбэю и эти машины. Как ожидается, поставка ударных вертолетов заказчику начнется в 2014 году. Тайвань получит самую последнюю версию Apache - AH-64D Block III.\\n'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines), train_lines[0].split('\\t')\n",
    "# тема\n",
    "# заголовок\n",
    "# содержание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'science': 2156,\n",
       "         'sport': 2215,\n",
       "         'economics': 2080,\n",
       "         'media': 2111,\n",
       "         'culture': 2053,\n",
       "         'life': 2033,\n",
       "         'travel': 289,\n",
       "         'style': 284,\n",
       "         'forces': 1225,\n",
       "         'business': 554})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([line.split('\\t')[0] for line in train_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://drive.google.com/file/d/1mG3tPS_59pANrgwd6T2IgnHWgph4vYbg/view?usp=sharing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "- pymorphy2\n",
    "- русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "- [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(string, regex_string=r'\\b\\w+\\b'):\n",
    "    return re.findall(regex_string, string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(lines, regex_string=r'\\b\\w+\\b'):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        label, title, text = line.split('\\t')\n",
    "        \n",
    "        title = tokenize_string(title, regex_string)\n",
    "        text = tokenize_string(text, regex_string)\n",
    "        \n",
    "        result.append(dict())\n",
    "        result[-1]['label'] = label\n",
    "        result[-1]['title'] = title\n",
    "        result[-1]['text'] = text\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_dataset(lines):\n",
    "    morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "    for line in tqdm(lines):\n",
    "        line['title'] = list(map(lambda x: morph_analyzer.parse(x)[0].normal_form, line['title']))\n",
    "        line['text'] = list(map(lambda x: morph_analyzer.parse(x)[0].normal_form, line['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'science',\n",
       " 'title': ['boeing', 'займется', 'техобслуживанием', 'тайваньских', 'апачей'],\n",
       " 'text': ['американский',\n",
       "  'авиастроительный',\n",
       "  'концерн',\n",
       "  'boeing',\n",
       "  'получил',\n",
       "  'контракт',\n",
       "  'на',\n",
       "  'техническое',\n",
       "  'обслуживание',\n",
       "  'ударных',\n",
       "  'вертолетов',\n",
       "  'ah',\n",
       "  '64d',\n",
       "  'apache',\n",
       "  'которые',\n",
       "  'в',\n",
       "  'перспективе',\n",
       "  'должны',\n",
       "  'получить',\n",
       "  'сухопутные',\n",
       "  'войска',\n",
       "  'тайваня',\n",
       "  'как',\n",
       "  'сообщает',\n",
       "  'defense',\n",
       "  'aerospace',\n",
       "  'сумма',\n",
       "  'сделки',\n",
       "  'составила',\n",
       "  '141',\n",
       "  '3',\n",
       "  'миллиона',\n",
       "  'долларов',\n",
       "  'речь',\n",
       "  'о',\n",
       "  'идет',\n",
       "  'о',\n",
       "  'техническом',\n",
       "  'обслуживании',\n",
       "  '30',\n",
       "  'машин',\n",
       "  'подробности',\n",
       "  'соглашения',\n",
       "  'не',\n",
       "  'раскрываются',\n",
       "  'как',\n",
       "  'ожидается',\n",
       "  'работы',\n",
       "  'по',\n",
       "  'контракту',\n",
       "  'завершатся',\n",
       "  'в',\n",
       "  'декабре',\n",
       "  '2017',\n",
       "  'года',\n",
       "  'тайвань',\n",
       "  'заказал',\n",
       "  'у',\n",
       "  'сша',\n",
       "  '30',\n",
       "  'вертолетов',\n",
       "  'apache',\n",
       "  'в',\n",
       "  '2008',\n",
       "  'году',\n",
       "  'помимо',\n",
       "  'этих',\n",
       "  'машин',\n",
       "  'в',\n",
       "  'заказ',\n",
       "  'также',\n",
       "  'вошла',\n",
       "  'поставка',\n",
       "  'многоцелевых',\n",
       "  'вертолетов',\n",
       "  'black',\n",
       "  'hawk',\n",
       "  'зенитных',\n",
       "  'ракетных',\n",
       "  'комплексов',\n",
       "  'patriot',\n",
       "  'и',\n",
       "  'истребителей',\n",
       "  'f',\n",
       "  '16',\n",
       "  'в',\n",
       "  'начале',\n",
       "  '2010',\n",
       "  'года',\n",
       "  'стало',\n",
       "  'известно',\n",
       "  'что',\n",
       "  'сша',\n",
       "  'частично',\n",
       "  'одобрили',\n",
       "  'продажу',\n",
       "  'военной',\n",
       "  'техники',\n",
       "  'тайваню',\n",
       "  'в',\n",
       "  'частности',\n",
       "  'было',\n",
       "  'решено',\n",
       "  'поставить',\n",
       "  'стране',\n",
       "  'комплексы',\n",
       "  'patriot',\n",
       "  'вертолеты',\n",
       "  'black',\n",
       "  'hawk',\n",
       "  'и',\n",
       "  'оборудование',\n",
       "  'для',\n",
       "  'связи',\n",
       "  'считалось',\n",
       "  'что',\n",
       "  'поставка',\n",
       "  'ah',\n",
       "  '64d',\n",
       "  'тайваню',\n",
       "  'была',\n",
       "  'заблокирована',\n",
       "  'однако',\n",
       "  'в',\n",
       "  '2010',\n",
       "  'году',\n",
       "  'стало',\n",
       "  'известно',\n",
       "  'что',\n",
       "  'сша',\n",
       "  'все',\n",
       "  'же',\n",
       "  'решили',\n",
       "  'продать',\n",
       "  'тайбэю',\n",
       "  'и',\n",
       "  'эти',\n",
       "  'машины',\n",
       "  'как',\n",
       "  'ожидается',\n",
       "  'поставка',\n",
       "  'ударных',\n",
       "  'вертолетов',\n",
       "  'заказчику',\n",
       "  'начнется',\n",
       "  'в',\n",
       "  '2014',\n",
       "  'году',\n",
       "  'тайвань',\n",
       "  'получит',\n",
       "  'самую',\n",
       "  'последнюю',\n",
       "  'версию',\n",
       "  'apache',\n",
       "  'ah',\n",
       "  '64d',\n",
       "  'block',\n",
       "  'iii']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = tokenize_dataset(train_lines)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f71dfc2eeb34db4b3761d2b6ed595c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'science',\n",
       " 'title': ['boeing', 'заняться', 'техобслуживание', 'тайваньский', 'апач'],\n",
       " 'text': ['американский',\n",
       "  'авиастроительный',\n",
       "  'концерн',\n",
       "  'boeing',\n",
       "  'получить',\n",
       "  'контракт',\n",
       "  'на',\n",
       "  'технический',\n",
       "  'обслуживание',\n",
       "  'ударный',\n",
       "  'вертолёт',\n",
       "  'ah',\n",
       "  '64d',\n",
       "  'apache',\n",
       "  'который',\n",
       "  'в',\n",
       "  'перспектива',\n",
       "  'должный',\n",
       "  'получить',\n",
       "  'сухопутный',\n",
       "  'войско',\n",
       "  'тайвань',\n",
       "  'как',\n",
       "  'сообщать',\n",
       "  'defense',\n",
       "  'aerospace',\n",
       "  'сумма',\n",
       "  'сделка',\n",
       "  'составить',\n",
       "  '141',\n",
       "  '3',\n",
       "  'миллион',\n",
       "  'доллар',\n",
       "  'речь',\n",
       "  'о',\n",
       "  'идти',\n",
       "  'о',\n",
       "  'технический',\n",
       "  'обслуживание',\n",
       "  '30',\n",
       "  'машина',\n",
       "  'подробность',\n",
       "  'соглашение',\n",
       "  'не',\n",
       "  'раскрываться',\n",
       "  'как',\n",
       "  'ожидаться',\n",
       "  'работа',\n",
       "  'по',\n",
       "  'контракт',\n",
       "  'завершиться',\n",
       "  'в',\n",
       "  'декабрь',\n",
       "  '2017',\n",
       "  'год',\n",
       "  'тайвань',\n",
       "  'заказать',\n",
       "  'у',\n",
       "  'сша',\n",
       "  '30',\n",
       "  'вертолёт',\n",
       "  'apache',\n",
       "  'в',\n",
       "  '2008',\n",
       "  'год',\n",
       "  'помимо',\n",
       "  'этот',\n",
       "  'машина',\n",
       "  'в',\n",
       "  'заказ',\n",
       "  'также',\n",
       "  'войти',\n",
       "  'поставка',\n",
       "  'многоцелевой',\n",
       "  'вертолёт',\n",
       "  'black',\n",
       "  'hawk',\n",
       "  'зенитный',\n",
       "  'ракетный',\n",
       "  'комплекс',\n",
       "  'patriot',\n",
       "  'и',\n",
       "  'истребитель',\n",
       "  'f',\n",
       "  '16',\n",
       "  'в',\n",
       "  'начало',\n",
       "  '2010',\n",
       "  'год',\n",
       "  'стать',\n",
       "  'известно',\n",
       "  'что',\n",
       "  'сша',\n",
       "  'частично',\n",
       "  'одобрить',\n",
       "  'продажа',\n",
       "  'военный',\n",
       "  'техника',\n",
       "  'тайвань',\n",
       "  'в',\n",
       "  'частность',\n",
       "  'быть',\n",
       "  'решить',\n",
       "  'поставить',\n",
       "  'страна',\n",
       "  'комплекс',\n",
       "  'patriot',\n",
       "  'вертолёт',\n",
       "  'black',\n",
       "  'hawk',\n",
       "  'и',\n",
       "  'оборудование',\n",
       "  'для',\n",
       "  'связь',\n",
       "  'считаться',\n",
       "  'что',\n",
       "  'поставка',\n",
       "  'ah',\n",
       "  '64d',\n",
       "  'тайвань',\n",
       "  'быть',\n",
       "  'заблокировать',\n",
       "  'однако',\n",
       "  'в',\n",
       "  '2010',\n",
       "  'год',\n",
       "  'стать',\n",
       "  'известно',\n",
       "  'что',\n",
       "  'сша',\n",
       "  'всё',\n",
       "  'же',\n",
       "  'решить',\n",
       "  'продать',\n",
       "  'тайбэя',\n",
       "  'и',\n",
       "  'этот',\n",
       "  'машина',\n",
       "  'как',\n",
       "  'ожидаться',\n",
       "  'поставка',\n",
       "  'ударный',\n",
       "  'вертолёт',\n",
       "  'заказчик',\n",
       "  'начаться',\n",
       "  'в',\n",
       "  '2014',\n",
       "  'год',\n",
       "  'тайвань',\n",
       "  'получить',\n",
       "  'самый',\n",
       "  'последний',\n",
       "  'версия',\n",
       "  'apache',\n",
       "  'ah',\n",
       "  '64d',\n",
       "  'block',\n",
       "  'iii']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_dataset(train_data)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. \n",
    "Можно использовать [gensim] (https://radimrehurek.com/gensim/models/word2vec.html). Продемонстрировать семантические ассоциации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>[boeing, заняться, техобслуживание, тайваньски...</td>\n",
       "      <td>[американский, авиастроительный, концерн, boei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sport</td>\n",
       "      <td>[сергей, карякин, выиграть, кубок, мир, по, ша...</td>\n",
       "      <td>[российский, шахматист, сергей, карякин, вперв...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>[врач, разрешить, пеле, зажечь, олимпийский, о...</td>\n",
       "      <td>[врач, разрешить, бывший, футболист, сборная, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>economics</td>\n",
       "      <td>[евросоюз, выделить, 125, миллион, евро, на, п...</td>\n",
       "      <td>[из, бюджет, евросоюз, выделить, 125, миллион,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>media</td>\n",
       "      <td>[сми, сообщить, о, продажа, павел, дуров, акци...</td>\n",
       "      <td>[основатель, и, генеральный, директор, социаль...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>forces</td>\n",
       "      <td>[бывший, президент, анталбанка, арестовать, по...</td>\n",
       "      <td>[бывший, президент, анталбанка, магомед, мухий...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>life</td>\n",
       "      <td>[британец, отказаться, покупать, книга, премье...</td>\n",
       "      <td>[книга, премьер, министр, великобритания, горд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>economics</td>\n",
       "      <td>[дочка, башнефть, лишить, лицензия, на, местор...</td>\n",
       "      <td>[роснедра, 18, май, отменить, свой, приказ, о,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>media</td>\n",
       "      <td>[угадать, мелодия, вернуться, в, эфир]</td>\n",
       "      <td>[программа, угадать, мелодия, в, январь, 2013,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>science</td>\n",
       "      <td>[в, япония, заявить, о, план, создание, собств...</td>\n",
       "      <td>[правительство, япония, заявить, о, план, по, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "0        science  [boeing, заняться, техобслуживание, тайваньски...   \n",
       "1          sport  [сергей, карякин, выиграть, кубок, мир, по, ша...   \n",
       "2          sport  [врач, разрешить, пеле, зажечь, олимпийский, о...   \n",
       "3      economics  [евросоюз, выделить, 125, миллион, евро, на, п...   \n",
       "4          media  [сми, сообщить, о, продажа, павел, дуров, акци...   \n",
       "...          ...                                                ...   \n",
       "14995     forces  [бывший, президент, анталбанка, арестовать, по...   \n",
       "14996       life  [британец, отказаться, покупать, книга, премье...   \n",
       "14997  economics  [дочка, башнефть, лишить, лицензия, на, местор...   \n",
       "14998      media             [угадать, мелодия, вернуться, в, эфир]   \n",
       "14999    science  [в, япония, заявить, о, план, создание, собств...   \n",
       "\n",
       "                                                    text  \n",
       "0      [американский, авиастроительный, концерн, boei...  \n",
       "1      [российский, шахматист, сергей, карякин, вперв...  \n",
       "2      [врач, разрешить, бывший, футболист, сборная, ...  \n",
       "3      [из, бюджет, евросоюз, выделить, 125, миллион,...  \n",
       "4      [основатель, и, генеральный, директор, социаль...  \n",
       "...                                                  ...  \n",
       "14995  [бывший, президент, анталбанка, магомед, мухий...  \n",
       "14996  [книга, премьер, министр, великобритания, горд...  \n",
       "14997  [роснедра, 18, май, отменить, свой, приказ, о,...  \n",
       "14998  [программа, угадать, мелодия, в, январь, 2013,...  \n",
       "14999  [правительство, япония, заявить, о, план, по, ...  \n",
       "\n",
       "[15000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(train_df['text'].append(train_df['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('малкин', 0.9228668212890625), ('питтсбург', 0.9222313165664673), ('голкипер', 0.9207319021224976), ('кириленко', 0.9102184772491455), ('испанец', 0.910172700881958), ('радулов', 0.9056620001792908), ('аргентинец', 0.8954154849052429), ('вратарь', 0.8910346627235413), ('надаль', 0.8870075941085815), ('пингвинс', 0.8808801174163818)] \n",
      "\n",
      "[('apple', 0.8774051666259766), ('lg', 0.8682652115821838), ('motorola', 0.8467772603034973), ('смартфон', 0.8339815735816956), ('планшет', 0.8321726322174072), ('nokia', 0.8281078934669495), ('microsoft', 0.8219476342201233), ('iphone', 0.8164066076278687), ('мобильный', 0.8061249256134033), ('htc', 0.8052313327789307)] \n",
      "\n",
      "[('динамо', 0.9529554843902588), ('цска', 0.9505517482757568), ('локомотив', 0.9247673749923706), ('зенит', 0.8965145945549011), ('анжи', 0.8618971705436707), ('полузащитник', 0.8587614297866821), ('челси', 0.8583778142929077), ('ска', 0.8508938550949097), ('краснодар', 0.8498367071151733), ('армеец', 0.8480013608932495)] \n",
      "\n",
      "[('android', 0.9436208009719849), ('windows', 0.9286665916442871), ('ipad', 0.8984448313713074), ('планшет', 0.8965500593185425), ('phone', 0.8857302069664001), ('iphone', 0.8821375966072083), ('прошивка', 0.8745062351226807), ('chrome', 0.8621495366096497), ('playstation', 0.8591451644897461), ('os', 0.8476467132568359)] \n",
      "\n",
      "[('казань', 0.7511264085769653), ('новосибирск', 0.743058443069458), ('севастополь', 0.7412639856338501), ('стокгольм', 0.7261931300163269), ('округ', 0.7220088243484497), ('симферополь', 0.7174490690231323), ('анталья', 0.7019568681716919), ('минск', 0.6985460519790649), ('назера', 0.6966593265533447), ('донбасс', 0.6867426633834839)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.wv.most_similar(positive=['овечкин']), '\\n')\n",
    "print(word2vec.wv.most_similar(positive=['samsung']), '\\n')\n",
    "print(word2vec.wv.most_similar(positive=['спартак']), '\\n')\n",
    "print(word2vec.wv.most_similar(positive=['ios']), '\\n')\n",
    "print(word2vec.wv.most_similar(positive=['владивосток']), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://sun9-14.userapi.com/impg/fz61Au_EA7v-8IQDq_ZEDqAc_ao4NNj44rd3CQ/vu1EETE-dG8.jpg?size=640x332&quality=96&proxy=1&sign=9835fa79d633abb6cfa2ef0495b8e0a5&type=album)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. \n",
    "Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "- SVM\n",
    "- наивный байесовский классификатор\n",
    "- логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'sport': 0,\n",
    "                'culture': 1,\n",
    "                'science': 2,\n",
    "                'media': 3,\n",
    "                'economics': 4,\n",
    "                'life': 5,\n",
    "                'forces': 6,\n",
    "                'travel': 7,\n",
    "                'style': 8,\n",
    "                'business': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(labels, mapping_dict):\n",
    "    return labels.map(lambda label: mapping_dict[label]).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_tfidf_vectorizer(X, word2vec):\n",
    "    word2vec_dict = dict(zip(word2vec.wv.index2word, word2vec.wv.vectors))\n",
    "    dim = len(next(iter(word2vec_dict.values())))        \n",
    "    \n",
    "    tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "    tfidf.fit(X)\n",
    "    max_idf = max(tfidf.idf_)\n",
    "    word2weight = defaultdict(\n",
    "        lambda: max_idf, \n",
    "        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "    return np.array([\n",
    "        np.mean([word2vec[w] * word2weight[w] \n",
    "                 for w in words if w in word2vec] or \n",
    "                [np.zeros(dim)], axis=0) \n",
    "        for words in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsark\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\tsark\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "x_train = mean_tfidf_vectorizer(train_df['text'], word2vec)\n",
    "y_train = map_labels(train_df['label'], mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97072351832040a2892caac95a518ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsark\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\tsark\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "test_lines = list(open(TEST_PATH, 'r', encoding='utf-8'))\n",
    "test_data = tokenize_dataset(test_lines)\n",
    "\n",
    "lemmatize_dataset(test_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "x_test = mean_tfidf_vectorizer(test_df['text'], word2vec)\n",
    "y_test = map_labels(test_df['label'], mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8413333333333334"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(n_jobs = -1)\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8516666666666667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
